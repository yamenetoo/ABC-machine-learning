{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTu2ngFABdMqQASeeE+b44",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yamenetoo/ABC-machine-learning/blob/main/Scikit-Learn/Sklearn_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1\tIntroduction\n",
        "\n",
        "In statistics and machine learning, metrics are quantitative measures or evaluation criteria used to assess the performance of a model. Metrics provide insights into how well a model is performing on a particular task, such as classification, regression, or clustering. The choice of metrics depends on the specific goals and characteristics of the problem at hand. Here are two main types of metrics:\n",
        "\n",
        "1. **Regression Metrics:** These metrics are used when the problem involves predicting a continuous variable.\n",
        "\n",
        "2. **Classification Metrics:**  These metrics are used when the problem involves categorizing data into classes or labels.\n",
        "\n",
        "Metrics help data scientists and machine learning practitioners to:\n",
        "\n",
        "- Quantify the performance of their models.\n",
        "- Compare different models or algorithms.\n",
        "- Identify areas for improvement and optimization.\n",
        "- Make informed decisions about model deployment.\n",
        "\n",
        "It's important to choose metrics that align with the goals and requirements of the specific machine learning task. Different metrics emphasize different aspects of model performance, and the selection should be tailored to the nature of the problem being addressed.\n",
        "2\t Regression Metrics\n",
        "\n",
        "2.1\t mean absolute error\n",
        "   - Definition: Mean Absolute Error (MAE) measures the average absolute differences between actual and predicted values.\n",
        "\n",
        "        $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
        "     Where \\(n\\) is the number of samples, $\\(y_i\\)$ is the actual value, and $\\(\\hat{y}_i\\)$ is the predicted value.\n",
        "\n",
        "   - Use Case: Commonly used for evaluating the performance of regression models.\n",
        "\n",
        "\n",
        "2.2\tmean squared error\n",
        "   - Definition: Mean Squared Error (MSE) calculates the average squared differences between actual and predicted values.\n",
        "\n",
        "        $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
        "\n",
        "\n",
        "   - Use Case: Similar to MAE but gives more weight to large errors.\n",
        "\n",
        "\n",
        "2.3\tmedian absolute error\n",
        "   - Definition: Median Absolute Error is the median of the absolute differences between actual and predicted values.\n",
        "\n",
        "       $$ MedAE = \\text{median}(|y_1 - \\hat{y}_1|, |y_2 - \\hat{y}_2|, ..., |y_n - \\hat{y}_n|) $$\n",
        "\n",
        "\n",
        "\n",
        "   - Use Case: Similar to MAE but less sensitive to outliers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#3\t Classification Metrics:\n",
        "\n",
        "##3.1\t confusion matrix\n",
        "   - Definition: Confusion Matrix is a table used to evaluate the performance of a classification algorithm, showing the counts of true positive, true negative, false positive, and false negative predictions.\n",
        "   - Use Case: Provides insights into the model's performance on different classes.\n",
        "     The confusion matrix is a table representing counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). It is not expressed as a single formula.\n",
        "\n",
        "##3.2\taccuracy score\n",
        "   - Definition: Accuracy Score measures the proportion of correctly classified instances out of the total instances.\n",
        "\n",
        "   $$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
        "\n",
        "   - Use Case: Commonly used for evaluating the overall performance of a classification model.\n",
        "     \n",
        "\n",
        "##3.3\tf1 score\n",
        "   - Definition: F1 Score is the harmonic mean of precision and recall, providing a balance between precision and recall.\n",
        "\n",
        "      $$ F1 = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
        "\n",
        "   - Use Case: Particularly useful when there is an uneven class distribution.\n",
        "  \n",
        "\n",
        "##3.4\trecall score\n",
        "   - Definition: Recall, or Sensitivity, measures the ability of a classifier to capture all relevant instances.\n",
        "\n",
        "     $$ Recall = \\frac{TP}{TP + FN} $$\n",
        "\n",
        "\n",
        "\n",
        "   - Use Case: Important when minimizing false negatives is crucial.\n",
        "##3.5\tprecision score\n",
        "   - Definition: Precision measures the accuracy of positive predictions, emphasizing the reliability of positive classifications.\n",
        "\n",
        "       $$ Precision = \\frac{TP}{TP + FP} $$\n",
        "\n",
        "\n",
        "   - Use Case: Important when minimizing false positives is crucial.\n",
        "     \n",
        "\n",
        "##3.6\tprecision recall fscore support\n",
        "   - Definition: Provides precision, recall, and F1-score for each class in a multi-class classification problem.\n",
        "   - Use Case: Offers detailed performance metrics for individual classes.\n",
        "\n",
        "##3.7\tprecision recall curve\n",
        "   - Definition: Plots precision and recall values for different probability thresholds.\n",
        "   - Use Case: Helps visualize the trade-off between precision and recall at different decision thresholds.\n",
        "\n",
        "##3.8\tclassification report\n",
        "   - Definition: Generates a text report with precision, recall, F1-score, and support for each class.\n",
        "   - Use Case: Summarizes classification metrics for multiple classes.\n",
        "\n",
        "##3.9\troc curve\n",
        "   - Definition: Receiver Operating Characteristic (ROC) curve plots the true positive rate against the false positive rate at various decision thresholds.\n",
        "   - Use Case: Visualizes the trade-off between sensitivity and specificity.\n",
        "\n",
        "##3.10\tauc\n",
        "   - Definition: Area Under the Curve (AUC) measures the area under the ROC\n",
        "   curve, providing a single value to summarize classifier performance.\n",
        "\n",
        "   $$ AUC = \\int_{0}^{1} \\text{TPR}(fpr) \\, \\text{d}fpr $$\n",
        "\n",
        "\n",
        "   - Use Case: Quantifies the overall performance of a classifier.\n",
        "     \n",
        "\n",
        "  \n",
        "##3.11\troc auc score\n",
        "   - Definition: Computes the AUC from the ROC curve.\n",
        "\n",
        "       $$ ROC \\, AUC = \\int_{0}^{1} \\text{TPR}(fpr) \\, \\text{d}fpr $$\n",
        "\n",
        "\n",
        "   - Use Case: Provides a single value to assess the classifier's discriminative ability.\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "##3.12\tzero one loss\n",
        "   - Definition: Zero-One Loss calculates the fraction of misclassifications.\n",
        "\n",
        "        $$ Zero-One \\, Loss = \\frac{FP + FN}{TP + TN + FP + FN} $$\n",
        "\n",
        "\n",
        "   - Use Case: Similar to accuracy but focuses on misclassifications.\n",
        "\n",
        "\n",
        "These metrics provide a comprehensive set of tools for evaluating the performance of both regression and classification models, covering various aspects of model quality. Choose the metric(s) that align with the specific goals and requirements of your analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PNDUGTQSjPUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "# 1\tمقدمه\n",
        "\n",
        "در آمار و یادگیری ماشین، معیارها اندازه‌گیری‌های کمی یا معیارهای ارزیابی هستند که برای ارزیابی عملکرد یک مدل استفاده می‌شوند. معیارها به تحلیل عملکرد یک مدل در یک وظیفه خاص مانند طبقه‌بندی، رگرسیون یا خوشه‌بندی کمک می‌کنند. انتخاب معیارها به موارد خاص و ویژگی‌های مسئله وابسته است. دو نوع اصلی معیار وجود دارد:\n",
        "\n",
        "1. **معیارهای رگرسیون:** این معیارها زمانی مورد استفاده قرار می‌گیرند که مسئله شامل پیش‌بینی یک متغیر پیوسته است.\n",
        "\n",
        "2. **معیارهای طبقه‌بندی:** این معیارها زمانی استفاده می‌شوند که مسئله شامل دسته‌بندی داده‌ها به کلاس‌ها یا برچسب‌ها است.\n",
        "\n",
        "معیارها به دیتاساینتیست‌ها و افراد علم داده کمک می‌کنند:\n",
        "\n",
        "- عملکرد مدل‌های خود را به شکل کمی ارزیابی کنند.\n",
        "- مدل‌ها یا الگوریتم‌های مختلف را مقایسه کنند.\n",
        "- نقاط قوت و ضعف مدل را شناسایی و بهینه‌سازی کنند.\n",
        "- تصمیمات آگاهانه درباره استقرار مدل بگیرند.\n",
        "\n",
        "مهم است که معیارهایی انتخاب شوند که با اهداف و الزامات وظیفه خاص یادگیری ماشین هماهنگ باشند. معیارهای مختلف نکات مختلفی از عملکرد مدل را تأکید می‌کنند و انتخاب باید با ماهیت مسئله مورد نظر هماهنگ شود.\n",
        "\n",
        "# 2\tمعیارهای رگرسیون\n",
        "\n",
        "## 2.1\tمیانگین خطای مطلق\n",
        "   - **تعریف:** میانگین خطای مطلق (MAE) میانگین اختلاف‌های مطلق بین مقادیر واقعی و پیش‌بینی شده است.\n",
        "\n",
        "        $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
        "     که \\(n\\) تعداد نمونه‌ها، \\(y_i\\) مقدار واقعی و \\(\\hat{y} i\\) مقدار پیش‌بینی شده است.\n",
        "\n",
        "   - **مورد استفاده:** معمولاً برای ارزیابی عملکرد مدل‌های رگرسیون استفاده می‌شود.\n",
        "\n",
        "## 2.2\tخطای میانگین مربعها\n",
        "   - **تعریف:** خطای میانگین مربعها (MSE) میانگین اختلاف‌های مربعی بین مقادیر واقعی و پیش‌بینی شده است.\n",
        "\n",
        "        $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
        "     که \\(n\\) تعداد نمونه‌ها، \\(y_i\\) مقدار واقعی و \\(\\hat{y} i\\) مقدار پیش‌بینی شده است.\n",
        "\n",
        "   - **مورد استفاده:** مشابه با MAE اما به خطاهای بزرگ وزن بیشتری می‌دهد.\n",
        "\n",
        "## 2.3\tخطای میانهٔ مطلق\n",
        "   - **تعریف:** خطای میانهٔ مطلق، میانه اختلافات مطلق بین مقادیر واقعی و پیش‌بینی شده است.\n",
        "\n",
        "       $$ MedAE = \\text{median}(|y_1 - \\hat{y}_1|, |y_2 - \\hat{y}_2|, ..., |y_n - \\hat{y}_n|) $$\n",
        "     که \\(n\\) تعداد نمونه‌ها، \\(y_i\\) مقدار واقعی و \\(\\hat{y}_i\\) مقدار پیش‌بینی شده است.\n",
        "\n",
        "   - **مورد استفاده:** مشابه با MAE اما به نوسانات ناگهانی کمتر حساس است.\n",
        "\n",
        "# 3\tمعیارهای طبقه‌بندی\n",
        "\n",
        "## 3.1\tماتریس گیجن\n",
        "   - **تعریف:** ماتریس گیجن یک جدول استفاده می‌شود برای ارزیابی عملکرد الگوریتم\n",
        "\n",
        " طبقه‌بندی، نمایش تعداد پیش‌بینی‌های واقعی مثبت، واقعی منفی، مثبت غلط و منفی غلط.\n",
        "   - **مورد استفاده:** برای درک عملکرد مدل در کلاس‌های مختلف.\n",
        "\n",
        "## 3.2\tامتیاز دقت\n",
        "   - **تعریف:** امتیاز دقت نسبت پیش‌بینی‌های صحیح به کل نمونه‌ها را اندازه‌گیری می‌کند.\n",
        "\n",
        "   $$ دقت = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
        "\n",
        "   - **مورد استفاده:** معمولاً برای ارزیابی عملکرد کلی یک مدل طبقه‌بندی استفاده می‌شود.\n",
        "\n",
        "## 3.3\tامتیاز F1\n",
        "   - **تعریف:** امتیاز F1 میانگین هندسی از دقت و بازخوانی است، تعادلی بین دقت و بازخوانی فراهم می‌کند.\n",
        "\n",
        "      $$ F1 = \\frac{2 \\times \\text{دقت} \\times \\text{بازخوانی}}{\\text{دقت} + \\text{بازخوانی}} $$\n",
        "\n",
        "   - **مورد استفاده:** به ویژه زمانی که توزیع ناهموار کلاس‌ها وجود دارد، مفید است.\n",
        "\n",
        "## 3.4\tامتیاز بازخوانی\n",
        "   - **تعریف:** بازخوانی یا حساسیت، توانایی یک طبقه‌بند در گرفتن همه نمونه‌های مربوط را اندازه‌گیری می‌کند.\n",
        "\n",
        "     $$ بازخوانی = \\frac{TP}{TP + FN} $$\n",
        "\n",
        "   - **مورد استفاده:** زمانی که کاهش تعداد منفی‌های غلط بسیار حیاتی است.\n",
        "\n",
        "## 3.5\tامتیاز دقت\n",
        "   - **تعریف:** دقت دقت پیش‌بینی‌های مثبت را اندازه‌گیری می‌کند و قابلیت پیش‌بینی مثبت‌ها را تأکید می‌کند.\n",
        "\n",
        "       $$ دقت = \\frac{TP}{TP + FP} $$\n",
        "\n",
        "   - **مورد استفاده:** زمانی که کاهش مثبت‌های غلط بسیار حیاتی است.\n",
        "\n",
        "## 3.6\tامتیاز دقت-بازخوانی-F1-پشتیبانی\n",
        "   - **تعریف:** ارائه دقت، بازخوانی و امتیاز F1 برای هر کلاس در یک مسئله طبقه‌بندی چندکلاسه.\n",
        "   - **مورد استفاده:** معیارهای عملکرد دقیق برای هر کلاس را ارائه می‌دهد.\n",
        "\n",
        "## 3.7\tنمودار دقت-بازخوانی\n",
        "   - **تعریف:** نمودار دقت و بازخوانی را برای آستانه‌های احتمال مختلف رسم می‌کند.\n",
        "   - **مورد استفاده:** به تصویر کشیدن تعادل میان دقت و بازخوانی در آستانه‌های تصمیم مختلف کمک می‌کند.\n",
        "\n",
        "## 3.8\tگزارش طبقه‌بندی\n",
        "   - **تعریف:** یک گزارش متنی با دقت، بازخوانی، امتیاز F1 و پشتیبانی برای هر کلاس تولید می‌کند.\n",
        "   - **مورد استفاده:** خلاصه‌ای از معیارهای طبقه‌بندی برای چندین کلاس را ارائه می‌دهد.\n",
        "\n",
        "## 3.9\tنمودار ROC\n",
        "   - **تعریف:** نمودار مشخصه عملکرد گیرنده عملکرد (ROC) نرخ واقعی مثبت را در برابر نرخ مثبت غلط در آستانه‌های تصمیم مختلف رسم می‌کند.\n",
        "   - **مورد استفاده:** تعادل میان حساسیت و اختصاص در آستانه‌های مختلف را بصری می‌کند.\n",
        "\n",
        "## 3.10\tمساحت زیر نمودار\n",
        "   - **تعریف:** مساحت زیر نمودار ROC را اندازه‌گیری می‌کند و یک مقدار تکی برای خلاصه عملکرد طبقه‌بندی ارائه می‌دهد.\n",
        "\n",
        "   $$ مساحت زیر نمودار = \\int_{0}^{1} \\text{TPR}(fpr) \\, \\text{d}fpr $$\n",
        "\n",
        "\n",
        "\n",
        "   - **مورد استفاده:** عملکرد کلی طبقه‌بندی را به صورت یک عدد تومانی مشخص می‌کند.\n",
        "\n",
        "## 3.11\tامتیاز ROC AUC\n",
        "   - **تعریف:** امتیاز ROC AUC از منحنی ROC محاسبه می‌شود.\n",
        "\n",
        "       $$ امتیاز ROC AUC = \\int_{0}^{1} \\text{TPR}(fpr) \\, \\text{d}fpr $$\n",
        "\n",
        "   - **مورد استفاده:** یک مقدار تکی برای ارزیابی توانایی تمییزدهی طبقه‌بندی ارائه می‌دهد.\n",
        "\n",
        "## 3.12\tخطای صفر یک\n",
        "   - **تعریف:** خطای صفر یک نسبت تعداد اشتباهات دسته‌بندی شده به تعداد کل نمونه‌ها را محاسبه می‌کند.\n",
        "\n",
        "        $$ خطای صفر یک = \\frac{FP + FN}{TP + TN + FP + FN} $$\n",
        "\n",
        "   - **مورد استفاده:** مشابه با دقت اما بر روی اشتباهات تمرکز دارد.\n",
        "\n",
        "این معیارها مجموعه جامعی از ابزارها برای ارزیابی عملکرد مدل‌های رگرسیون و طبقه‌بندی فراهم می‌کنند و جوانب مختلفی از کیفیت مدل را پوشش می‌دهند. معیار(های) را که با اهداف و الزامات ویژه وظیفه خاص یادگیری ماشین هماهنگ هستند، انتخاب کنید.\n",
        "</div>"
      ],
      "metadata": {
        "id": "z7MoYacYl-l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
        "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve, classification_report\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, zero_one_loss"
      ],
      "metadata": {
        "id": "3jzETH7XlKbc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data\n",
        "# for regrassion\n",
        "y_true_regression = [2.5, 1.5, 3.0, 7.0]\n",
        "y_pred_regression = [3.0, 1.5, 2.5, 6.5]\n",
        "\n",
        "# for classification\n",
        "y_true_classification = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
        "y_pred_proba_classification = [0.8, 0.3, 0.9, 0.6, 0.2, 0.7, 0.1, 0.4, 0.85, 0.92]\n",
        "y_pred_classification = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n"
      ],
      "metadata": {
        "id": "yTGExHlXlLzH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Regression Metrics\n",
        "\n",
        "# 2.1 mean absolute error\n",
        "mae = mean_absolute_error(y_true_regression, y_pred_regression)\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "\n",
        "# 2.2 mean squared error\n",
        "mse = mean_squared_error(y_true_regression, y_pred_regression)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# 2.3 median absolute error\n",
        "medae = median_absolute_error(y_true_regression, y_pred_regression)\n",
        "print(f'Median Absolute Error: {medae}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V6ArDIJWjPzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Metrics\n",
        "\n",
        "# 3.1 confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true_classification, y_pred_classification)\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "\n",
        "# 3.2 accuracy score\n",
        "accuracy = accuracy_score(y_true_classification, y_pred_classification)\n",
        "print(f'Accuracy Score: {accuracy}')\n",
        "\n",
        "# 3.3 f1 score\n",
        "f1 = f1_score(y_true_classification, y_pred_classification)\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "# 3.4 recall score\n",
        "recall = recall_score(y_true_classification, y_pred_classification)\n",
        "print(f'Recall Score: {recall}')\n",
        "\n",
        "# 3.5 precision score\n",
        "precision = precision_score(y_true_classification, y_pred_classification)\n",
        "print(f'Precision Score: {precision}')\n",
        "\n",
        "# 3.6 precision recall fscore support\n",
        "precision_recall_fscore = precision_recall_fscore_support(y_true_classification, y_pred_classification)\n",
        "print(f'Precision, Recall, F1-Score, Support:\\n{precision_recall_fscore}')\n",
        "\n",
        "# 3.7 precision recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_true_classification, y_pred_proba_classification)\n",
        "\n",
        "# 3.8 classification report\n",
        "class_report = classification_report(y_true_classification, y_pred_classification)\n",
        "print(f'Classification Report:\\n{class_report}')\n",
        "\n",
        "# 3.9 roc curve\n",
        "fpr, tpr, thresholds = roc_curve(y_true_classification, y_pred_proba_classification)\n",
        "\n",
        "# 3.10 auc\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(f'AUC (Area Under the Curve): {roc_auc}')\n",
        "\n",
        "# 3.11 roc auc score\n",
        "roc_auc_score_value = roc_auc_score(y_true_classification, y_pred_proba_classification)\n",
        "print(f'ROC AUC Score: {roc_auc_score_value}')\n",
        "\n",
        "# 3.12 zero one loss\n",
        "zero_one_loss_value = zero_one_loss(y_true_classification, y_pred_classification)\n",
        "print(f'Zero-One Loss: {zero_one_loss_value}')"
      ],
      "metadata": {
        "id": "bYi_GhZ2mXXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}